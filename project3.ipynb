{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2342813f",
   "metadata": {},
   "source": [
    "## Student Name: \n",
    "## Student Email:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "84e6ab65",
   "metadata": {},
   "source": [
    "# Project 3: The Smart City Slicker\n",
    "\n",
    "Imagine you are a stakeholder in a rising Smart City and want to know more about themes and concepts about existing smart cities. You also want to know where does your smart city place among others. In this project, you will perform \n",
    "exploratory data analysis, often shortened to EDA, to examine a data from the [2015 Smart City Challenge](https://www.transportation.gov/smartcity) to find facts about the data and communicating those facts through text analysis and visualizations.\n",
    "\n",
    "In order to explore the data and visualize it, some modifications might need to be made to the data along the way. This is often referred to as data preprocessing or cleaning.\n",
    "Though data preprocessing is technically different from EDA, EDA often exposes problems with the data that need to be fixed in order to continue exploring.\n",
    "Because of this tight coupling, you have to clean the data as necessary to help understand the data.\n",
    "\n",
    "In this project, you will apply your knowledge about data cleaning, machine learning, visualizations, and databases to explore smart city applications.\n",
    "\n",
    "**Part 1** of the notebook will explore and clean the data. \\\n",
    "**Part 2** will take the results of the preprocessed data to create models and visualizations.\n",
    "\n",
    "Empty cells are code cells. \n",
    "Cells denoted with [Your Answer Here] are markdown cells.\n",
    "Edit and add as many cells as needed."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6e8dcba1",
   "metadata": {},
   "source": [
    "Output file for this notebook is shown as a table for display purposes. Note: The city name can be Norman, OK or OK Norman.\n",
    "\n",
    "| city | raw text | clean text | clusterid | topicids | summary | keywords|\n",
    "| -- | -- | -- | -- | -- | -- | -- |\n",
    "|Norman, OK | Test, test , and testing. | test test test | 0 | T1, T2| test | test |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89fd47ce",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "The Dataset: 2015 Smart City Challenge Applicants (non-finalist).\n",
    "In this project you will use the applicant's PDFs as a dataset.\n",
    "The dataset is from the U.S Department of Transportation Smart City Challenge.\n",
    "\n",
    "On the website page for the data, you can find some basic information about the challenge. This is an interesting dataset. Think of the questions that you might be able to answer! A few could be:\n",
    "\n",
    "1. Can I identify frequently occurring words that could be removed during data preprocessing?\n",
    "2. Where are the applicants from?\n",
    "3. Are there multiple entries for the same city in different applicantions?\n",
    "4. What are the major themes and concepts from the smart city applicants?\n",
    "\n",
    "Let's load the data!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7aace966",
   "metadata": {},
   "source": [
    "## Loading and Handling files\n",
    "\n",
    "Load data from `smartcity/`. \n",
    "\n",
    "To extract the data from the pdf files, use the [pypdf.pdf.PdfFileReader](https://pypdf.readthedocs.io/en/stable/index.html) class.\n",
    "It will allow you to extract pages and pdf files and add them to a data structure (dataframe, list, dictionary, etc).\n",
    "To install the module, use the command `pipenv install pypdf`.\n",
    "You only need to handle PDF files, handling docx is not necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0afb30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PyPDF2 import PdfReader\n",
    "\n",
    "pdf_dir = 'smartcity/'\n",
    "all_text = []\n",
    "\n",
    "# Loop over all files in the PDF directory\n",
    "for filename in os.listdir(pdf_dir):\n",
    "    # Check if the file is a PDF file\n",
    "    if filename.endswith('.pdf'):\n",
    "        # Open the PDF file\n",
    "        with open(os.path.join(pdf_dir, filename), 'rb') as pdf_file:\n",
    "            # Create a PdfReader object\n",
    "            pdf_reader = PdfReader(pdf_file)\n",
    "            # Loop over each page in the PDF file\n",
    "            for page_num in range(len(pdf_reader.pages)):\n",
    "                # Extract the text from the page\n",
    "                page_text = pdf_reader.pages[page_num].extract_text()\n",
    "\n",
    "                # Add the text to the list of text from all files\n",
    "                all_text.append(page_text)\n",
    "\n",
    "\n",
    "# Print the text from all files\n",
    "print(all_text)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d8ed6e32",
   "metadata": {},
   "source": [
    "Create a data structure to add the city name and raw text. You can choose to split the city name from the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4905f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PyPDF2 import PdfReader\n",
    "\n",
    "pdf_dir = 'smartcity/'\n",
    "data = []\n",
    "\n",
    "# Loop over all files in the PDF directory\n",
    "for filename in os.listdir(pdf_dir):\n",
    "    # Check if the file is a PDF file\n",
    "    if filename.endswith('.pdf'):\n",
    "        # Extract the city name from the filename\n",
    "        city_name = filename.split('.')[0]\n",
    "        # Open the PDF file\n",
    "        with open(os.path.join(pdf_dir, filename), 'rb') as pdf_file:\n",
    "            # Create a PdfReader object\n",
    "            pdf_reader = PdfReader(pdf_file)\n",
    "            # Extract the raw text from the PDF file\n",
    "            raw_text = ''\n",
    "            for page in pdf_reader.pages:\n",
    "                raw_text += page.extract_text()\n",
    "            # Add the data to the list of dictionaries\n",
    "            data.append({\n",
    "                'city': city_name,\n",
    "                'text': raw_text\n",
    "            })\n",
    "\n",
    "# Print the data\n",
    "print(data)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5019a8c3",
   "metadata": {},
   "source": [
    "## Cleaning Up PDFs\n",
    "\n",
    "One of the more frustrating aspects of PDF is loading the data into a readable format. The first order of business will be to preprocess the data. To start, you can use code provided by Text Analytics with Python, [Chapter 3](https://github.com/dipanjanS/text-analytics-with-python/blob/master/New-Second-Edition/Ch03%20-%20Processing%20and%20Understanding%20Text/Ch03a%20-%20Text%20Wrangling.ipynb): [contractions.py](https://github.com/dipanjanS/text-analytics-with-python/blob/master/New-Second-Edition/Ch05%20-%20Text%20Classification/contractions.py) (Pages 136-137), and [text_normalizer.py](https://github.com/dipanjanS/text-analytics-with-python/blob/master/New-Second-Edition/Ch05%20-%20Text%20Classification/text_normalizer.py) (Pages 155-156). Feel free to download the scripts or add the code directly to the notebook (please note this code is performed on dataframes).\n",
    "\n",
    "In addition to the data cleaning provided by the textbook, you will need to:\n",
    "1. Consider removing terms that may effect clustering and topic modeling. Words to consider are cities, states, common words (smart, city, page, etc.). Keep in mind n-gram combinations are important; this can also be revisited later depending on your model's performance.\n",
    "2. Check the data to remove applicants that text was not processed correctly. Do not remove more than 15 cities from the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8142e498",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PyPDF2 import PdfFileReader\n",
    "import re\n",
    "from contractions import CONTRACTION_MAP\n",
    "from text_normalizer import normalize_corpus\n",
    "\n",
    "def load_and_clean_pdfs(directory):\n",
    "    data = []\n",
    "    cities_to_remove = []\n",
    "    \n",
    "    # Iterate through all PDF files in the directory\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".pdf\"):\n",
    "            filepath = os.path.join(directory, filename)\n",
    "            \n",
    "            # Extract city name from file name\n",
    "            city = filename[:-4]\n",
    "            \n",
    "            # Extract raw text from PDF file\n",
    "            with open(filepath, \"rb\") as f:\n",
    "                pdf_reader = PdfFileReader(f)\n",
    "                raw_text = \"\"\n",
    "                for i in range(pdf_reader.getNumPages()):\n",
    "                    page = pdf_reader.getPage(i)\n",
    "                    raw_text += page.extractText()\n",
    "            \n",
    "            # Clean raw text\n",
    "            clean_text = normalize_text(raw_text)\n",
    "            \n",
    "            # Check if text was not processed correctly\n",
    "            if clean_text.strip() == \"\":\n",
    "                cities_to_remove.append(city)\n",
    "                continue\n",
    "            \n",
    "            # Remove terms that may affect clustering and topic modeling\n",
    "            clean_text = remove_terms(clean_text)\n",
    "            \n",
    "            # Add city name, raw text, and clean text to data list\n",
    "            data.append({\"city\": city, \"raw text\": raw_text, \"clean text\": clean_text})\n",
    "            \n",
    "            # Check if more than 15 cities have been removed\n",
    "            if len(cities_to_remove) > 15:\n",
    "                break\n",
    "    \n",
    "    # Remove cities that were not processed correctly\n",
    "    data = [d for d in data if d[\"city\"] not in cities_to_remove]\n",
    "    \n",
    "    return data\n",
    "\n",
    "def normalize_text(text):\n",
    "    # Apply text normalization using contractions and text normalizer scripts\n",
    "    # Code from Text Analytics with Python, Chapter 3: contractions.py (Pages 136-137)\n",
    "    # and text_normalizer.py (Pages 155-156)\n",
    "    text = expand_contractions(text, CONTRACTION_MAP)\n",
    "    text = normalize_corpus(text)\n",
    "    return text\n",
    "\n",
    "def expand_contractions(text, contraction_map):\n",
    "    # Function to expand contractions in the text\n",
    "    # Code from Text Analytics with Python, Chapter 3: contractions.py (Pages 136-137)\n",
    "    contraction_pattern = re.compile('({})'.format('|'.join(contraction_map.keys())), \n",
    "                                     flags=re.IGNORECASE|re.DOTALL)\n",
    "    \n",
    "    def expand_match(contraction):\n",
    "        match = contraction.group(0)\n",
    "        first_char = match[0]\n",
    "        expanded_contraction = contraction_map.get(match)\\\n",
    "                                   if contraction_map.get(match)\\\n",
    "                                   else contraction_map.get(match.lower())                       \n",
    "        expanded_contraction = first_char+expanded_contraction[1:]\n",
    "        return expanded_contraction\n",
    "    \n",
    "    expanded_text = contraction_pattern.sub(expand_match, text)\n",
    "    expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
    "    return expanded_text\n",
    "\n",
    "def remove_terms(text):\n",
    "    # Function to remove terms that may affect clustering and topic modeling\n",
    "    # You can customize this function based on your specific requirements\n",
    "    # For example, removing cities, states, and common words\n",
    "    terms_to_remove = ['city', 'state', 'smart', 'page']\n",
    "    for term in terms_to_remove:\n",
    "        text = text.replace(term, \"\")\n",
    "    return text\n",
    "\n",
    "# Example usage\n",
    "data = load_and_clean_pdfs(\"smartcity\")\n",
    "print(data)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1473a3e3",
   "metadata": {},
   "source": [
    "#### Add the cleaned text to the structure you created.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3737fe02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean_pdfs(directory):\n",
    "    data = []\n",
    "    cities_to_remove = []\n",
    "    \n",
    "    # Iterate through all PDF files in the directory\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".pdf\"):\n",
    "            filepath = os.path.join(directory, filename)\n",
    "            \n",
    "            # Extract city name from file name\n",
    "            city = filename[:-4]\n",
    "            \n",
    "            # Extract raw text from PDF file\n",
    "            with open(filepath, \"rb\") as f:\n",
    "                pdf_reader = PdfFileReader(f)\n",
    "                raw_text = \"\"\n",
    "                for i in range(pdf_reader.getNumPages()):\n",
    "                    page = pdf_reader.getPage(i)\n",
    "                    raw_text += page.extractText()\n",
    "            \n",
    "            # Clean raw text\n",
    "            clean_text = normalize_text(raw_text)\n",
    "            \n",
    "            # Check if text was not processed correctly\n",
    "            if clean_text.strip() == \"\":\n",
    "                cities_to_remove.append(city)\n",
    "                continue\n",
    "            \n",
    "            # Remove terms that may affect clustering and topic modeling\n",
    "            clean_text = remove_terms(clean_text)\n",
    "            \n",
    "            # Add city name, raw text, and clean text to data list\n",
    "            data.append({\"city\": city, \"raw text\": raw_text, \"clean text\": clean_text})\n",
    "            \n",
    "            # Check if more than 15 cities have been removed\n",
    "            if len(cities_to_remove) > 15:\n",
    "                break\n",
    "    \n",
    "    # Remove cities that were not processed correctly\n",
    "    data = [d for d in data if d[\"city\"] not in cities_to_remove]\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Example usage\n",
    "data = load_and_clean_pdfs(\"smartcity\")\n",
    "print(data)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "82cc947b",
   "metadata": {},
   "source": [
    "### Clean Up: Discussion\n",
    "Answer the questions below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1ba98d",
   "metadata": {},
   "source": [
    "#### Which Smart City applicants did you remove? What issues did you see with the documents?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ffebf5a5",
   "metadata": {},
   "source": [
    " I removed the Smart City applicants that had empty or blank text after the cleaning process. These documents were identified based on the condition clean_text.strip() == \"\", where clean_text represents the processed text of each document. The issues with these documents were that they either contained no meaningful content or the cleaning process resulted in the removal of all relevant information, making them irrelevant for further analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1620ed74",
   "metadata": {},
   "source": [
    "#### Explain what additional text processing methods you used and why."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ae42fc81",
   "metadata": {},
   "source": [
    "In addition to the basic text cleaning steps such as removing punctuation and converting to lowercase, I used the following additional text processing methods:\n",
    "\n",
    "Lemmatization: I applied lemmatization to reduce words to their base or dictionary form. This helps in normalizing the text and reducing the dimensionality of the data. It improves the accuracy of topic modeling and keyword extraction by treating different inflected forms of a word as a single entity.\n",
    "\n",
    "Stop-word Removal: I removed common stop words that do not carry significant meaning, such as \"the,\" \"is,\" \"in,\" etc. This step helps to eliminate noise and reduce the dimensionality of the data, focusing on more meaningful and informative words.\n",
    "\n",
    "Custom Term Removal: I removed additional terms that were specific to the Smart City domain and may not contribute much to the analysis. These terms, such as \"city,\" \"state,\" \"smart,\" and \"page,\" were identified based on domain knowledge and the specific requirements of the analysis.\n",
    "\n",
    "These additional text processing methods were used to improve the quality of the text data, remove noise, and focus on meaningful words for topic modeling and keyword extraction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15817355",
   "metadata": {},
   "source": [
    "#### Did you identify any potientally problematic words?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e0ad6082",
   "metadata": {},
   "source": [
    "Based on the output, there were no explicitly mentioned potentially problematic words. However, it is important to review the data and output in detail to identify any words or terms that may cause issues or misinterpretations. It is always recommended to have domain knowledge and context understanding to identify potentially problematic words or terms in specific applications or analyses."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a1507fbe",
   "metadata": {},
   "source": [
    "## Experimenting with Clustering Models\n",
    "\n",
    "Now, you'll start to explore models to find the optimal clustering model. In this section, you'll explore [K-means](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html), [Hierarchical](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html), and [DBSCAN](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html#sklearn.cluster.DBSCAN) clustering algorithms.\n",
    "Create these algorithms with k_clusters for K-means and Hierarchical.\n",
    "For each cell in the table provide the [Silhouette score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html#sklearn.metrics.silhouette_score), [Calinski and Harabasz score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.calinski_harabasz_score.html#sklearn.metrics.calinski_harabasz_score), and [Davies-Bouldin score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.davies_bouldin_score.html#sklearn.metrics.davies_bouldin_score).\n",
    "\n",
    "In each cell, create an array to store the values.\n",
    "For example, \n",
    "\n",
    "|Algorithm| k = 9 | k = 18| k = 36 | Optimal k| \n",
    "|--|--|--|--|--|\n",
    "|K-means| [S,CH,DB]| [S,CH,DB] | [S,CH,DB] | [S,CH,DB] |\n",
    "|Hierarchical |[S,CH,DB]| [S,CH,DB]| [S,CH,DB] | [S,CH,DB]|\n",
    "|DBSCAN | X | X | X | [S,CH,DB] |\n",
    "\n",
    "\n",
    "\n",
    "### Optimality \n",
    "You will need to find the optimal k for K-means and Hierarchical algorithms.\n",
    "Find the optimality for k in the range 2 to 50.\n",
    "Provide the code used to generate the optimal k and provide justification for your approach.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da2b033",
   "metadata": {},
   "source": [
    "|Algorithm| k = 9 | k = 18| k = 36 | Optimal k| \n",
    "|--|--|--|--|--|\n",
    "|K-means|--|--|--|--|\n",
    "|Hierarchical |--|--|--|--|\n",
    "|DBSCAN | X | X | X | -- |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc8c7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "\n",
    "def find_optimal_k(data, min_k, max_k):\n",
    "    optimal_k = None\n",
    "    max_silhouette_score = -1\n",
    "    \n",
    "    for k in range(min_k, max_k+1):\n",
    "        kmeans = KMeans(n_clusters=k, random_state=0)\n",
    "        labels = kmeans.fit_predict(data)\n",
    "        silhouette = silhouette_score(data, labels)\n",
    "        \n",
    "        if silhouette > max_silhouette_score:\n",
    "            max_silhouette_score = silhouette\n",
    "            optimal_k = k\n",
    "    \n",
    "    return optimal_k\n",
    "\n",
    "def evaluate_clustering(data, k):\n",
    "    kmeans = KMeans(n_clusters=k, random_state=0)\n",
    "    hierarchical = AgglomerativeClustering(n_clusters=k)\n",
    "    dbscan = DBSCAN()\n",
    "    \n",
    "    kmeans_labels = kmeans.fit_predict(data)\n",
    "    hierarchical_labels = hierarchical.fit_predict(data)\n",
    "    dbscan_labels = dbscan.fit_predict(data)\n",
    "    \n",
    "    silhouette_kmeans = silhouette_score(data, kmeans_labels)\n",
    "    silhouette_hierarchical = silhouette_score(data, hierarchical_labels)\n",
    "    silhouette_dbscan = silhouette_score(data, dbscan_labels)\n",
    "    \n",
    "    calinski_harabasz_kmeans = calinski_harabasz_score(data, kmeans_labels)\n",
    "    calinski_harabasz_hierarchical = calinski_harabasz_score(data, hierarchical_labels)\n",
    "    calinski_harabasz_dbscan = calinski_harabasz_score(data, dbscan_labels)\n",
    "    \n",
    "    davies_bouldin_kmeans = davies_bouldin_score(data, kmeans_labels)\n",
    "    davies_bouldin_hierarchical = davies_bouldin_score(data, hierarchical_labels)\n",
    "    davies_bouldin_dbscan = davies_bouldin_score(data, dbscan_labels)\n",
    "    \n",
    "    return [[silhouette_kmeans, calinski_harabasz_kmeans, davies_bouldin_kmeans],\n",
    "            [silhouette_hierarchical, calinski_harabasz_hierarchical, davies_bouldin_hierarchical],\n",
    "            [silhouette_dbscan, calinski_harabasz_dbscan, davies_bouldin_dbscan]]\n",
    "\n",
    "# Perform clustering and evaluation for each k value\n",
    "k_values = [9, 18, 36]\n",
    "\n",
    "# Optimal k for K-means\n",
    "optimal_k_kmeans = find_optimal_k(data, 2, 50)\n",
    "\n",
    "# Optimal k for Hierarchical\n",
    "optimal_k_hierarchical = find_optimal_k(data, 2, 50)\n",
    "\n",
    "# Evaluate clustering algorithms\n",
    "results = []\n",
    "for k in k_values:\n",
    "    result = evaluate_clustering(data, k)\n",
    "    results.append(result)\n",
    "\n",
    "# Print the results\n",
    "print(\"Algorithm\\tk = 9\\t\\tk = 18\\t\\tk = 36\\t\\tOptimal k\")\n",
    "print(\"K-means\\t\\t{}\\t\\t{}\\t\\t{}\\t\\t{}\".format(results[0][0], results[1][0], results[2][0], \"-\"))\n",
    "print(\"Hierarchical\\t{}\\t\\t{}\\t\\t{}\\t\\t{}\".format(results[0][1], results[1][1], results[2][1], \"-\"))\n",
    "print(\"DBSCAN\\t\\t{}\\t\\t{}\\t\\t{}\\t\\t{}\".format(results[0][2], results[1][2], results[2][2], \"-\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d96d23a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6c20fd69",
   "metadata": {},
   "source": [
    "#### How did you approach finding the optimal k?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "753c54da",
   "metadata": {},
   "source": [
    "When finding the optimal k, I used the K-means algorithm and the Silhouette score as the evaluation metric. The Silhouette score measures how well each data point fits its assigned cluster, with higher values indicating better-defined clusters. I iterated through a range of k values and selected the k that resulted in the highest Silhouette score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79ec635",
   "metadata": {},
   "source": [
    "#### What algorithm do you believe is the best? Why?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3136b0a4",
   "metadata": {},
   "source": [
    "Regarding the best algorithm, it depends on the specific characteristics of the data and the problem at hand. In this case, since we are dealing with text data from smart city documents, it's difficult to determine the best algorithm without further information. However, K-means and Hierarchical clustering are commonly used for text data clustering. K-means is computationally efficient and suitable for a large number of samples, while Hierarchical clustering can capture hierarchical relationships between clusters. It's recommended to evaluate the performance of both algorithms using appropriate metrics and domain knowledge to make an informed decision."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2e45a2a3",
   "metadata": {},
   "source": [
    "### Add Cluster ID to output file\n",
    "In your data structure, add the cluster id for each smart city respectively. Show the to append the clusterid code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad83e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PyPDF2 import PdfFileReader\n",
    "import argparse\n",
    "\n",
    "# Import contractions and text_normalizer functions from the provided scripts\n",
    "from contractions import CONTRACTION_MAP\n",
    "from text_normalizer import normalize_corpus\n",
    "\n",
    "# Function to load and clean PDFs\n",
    "def load_and_clean_pdfs(directory):\n",
    "    data = []\n",
    "    # Iterate through all PDF files in the directory\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".pdf\"):\n",
    "            filepath = os.path.join(directory, filename)\n",
    "            # Extract city name from file name\n",
    "            city = filename[:-4]\n",
    "            # Extract raw text from PDF file\n",
    "            with open(filepath, \"rb\") as f:\n",
    "                pdf_reader = PdfFileReader(f)\n",
    "                raw_text = \"\"\n",
    "                for i in range(pdf_reader.getNumPages()):\n",
    "                    page = pdf_reader.getPage(i)\n",
    "                    raw_text += page.extractText()\n",
    "            # Clean raw text using text_normalizer module\n",
    "            clean_text = normalize_corpus(raw_text)\n",
    "            # Add city name, raw text, and clean text to data list\n",
    "            data.append({\"city\": city, \"raw text\": raw_text, \"clean text\": clean_text})\n",
    "    return data\n",
    "\n",
    "# Function to write city data to output file\n",
    "def write_city_to_output_file(city, raw_text, clean_text, cluster_id, summary, keywords, output_file):\n",
    "    # Write new city to output file as a tab-separated row with cluster ID\n",
    "    with open(output_file, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"{}\\t{}\\t{}\\t{}\\t{}\\t{}\\n\".format(city, raw_text, clean_text, cluster_id, summary, keywords))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Define argparse to handle command line arguments\n",
    "    parser = argparse.ArgumentParser(description='Predict the type of cluster a new smart city document belongs to.')\n",
    "    parser.add_argument('--document', type=str, required=True, help='Name of the new smart city document to be predicted (including path if not in current directory)')\n",
    "    parser.add_argument('--summarize', action='store_true', help='Include this argument to generate a summary for the new document')\n",
    "    parser.add_argument('--keywords', action='store_true', help='Include this argument to extract keywords from the new document')\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Load and clean PDFs from \"smartcity\" directory\n",
    "    data = load_and_clean_pdfs(\"smartcity\")\n",
    "\n",
    "    # Perform clustering and obtain cluster IDs for each smart city\n",
    "    cluster_ids = [0, 1, 2]  # Replace with the actual cluster IDs\n",
    "\n",
    "    # Iterate over the data structure and append cluster ID to the output file\n",
    "    output_file = \"smartcity_predict.tsv\"  # Replace with the path to your output file\n",
    "    for i, city_data in enumerate(data):\n",
    "        city = city_data[\"city\"]\n",
    "        raw_text = city_data[\"raw text\"]\n",
    "        clean_text = city_data[\"clean text\"]\n",
    "        cluster_id = cluster_ids[i]\n",
    "        summary = city_data[\"summary\"] if args.summarize else \"\"\n",
    "        keywords = city_data[\"keywords\"] if args.keywords else \"\"\n",
    "\n",
    "        write_city_to_output_file(city, raw_text, clean_text, cluster_id, summary, keywords, output_file)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "959e7275",
   "metadata": {},
   "source": [
    "### Save Model\n",
    "\n",
    "After finding the best model, it is desirable to have a way to persist the model for future use without having to retrain. Save the model using [model persistance](https://scikit-learn.org/stable/model_persistence.html). This model should be saved in the same directory as this notebook and should be loaded as the model for your `project3.py`.\n",
    "\n",
    "Save the model as `model.pkl`. You do not have to use pickle, but be sure to save the persistance using one of the methods listed in the link."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c80938e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the K-means model\n",
    "import joblib\n",
    "\n",
    "joblib.dump(kmeans, 'model.pkl')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3fe5a0c9",
   "metadata": {},
   "source": [
    "## Derving Themes and Concepts\n",
    "\n",
    "Perform Topic Modeling on the cleaned data. Provide the top five words for `TOPIC_NUM = Best_k` as defined in the section above. Feel free to reference [Chapter 6](https://github.com/dipanjanS/text-analytics-with-python/tree/master/New-Second-Edition/Ch06%20-%20Text%20Summarization%20and%20Topic%20Models) for more information on Topic Modeling and Summarization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b684bc24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models\n",
    "\n",
    "# Create a list of tokenized documents\n",
    "documents = [doc['clean text'] for doc in data]\n",
    "\n",
    "# Create a dictionary of all the unique words in the documents\n",
    "dictionary = corpora.Dictionary(documents)\n",
    "\n",
    "# Convert the documents into a document-term matrix\n",
    "corpus = [dictionary.doc2bow(doc) for doc in documents]\n",
    "\n",
    "# Define the number of topics\n",
    "num_topics = <number_of_topics>\n",
    "\n",
    "# Train the LDA model\n",
    "lda_model = models.LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=10)\n",
    "\n",
    "# Get the top five words for each topic\n",
    "top_words_per_topic = []\n",
    "for topic_id in range(num_topics):\n",
    "    top_words = lda_model.show_topic(topic_id, topn=5)\n",
    "    top_words_per_topic.append([word for word, _ in top_words])\n",
    "\n",
    "# Print the top five words for each topic\n",
    "for i, top_words in enumerate(top_words_per_topic):\n",
    "    print(f\"Topic {i + 1}: {' '.join(top_words)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "915516dc",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'clean text'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgensim\u001b[39;00m \u001b[39mimport\u001b[39;00m corpora, models\n\u001b[1;32m      3\u001b[0m \u001b[39m# Create a list of tokenized documents\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m documents \u001b[39m=\u001b[39m [doc[\u001b[39m'\u001b[39m\u001b[39mclean text\u001b[39m\u001b[39m'\u001b[39m] \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m data]\n\u001b[1;32m      6\u001b[0m \u001b[39m# Create a dictionary of all the unique words in the documents\u001b[39;00m\n\u001b[1;32m      7\u001b[0m dictionary \u001b[39m=\u001b[39m corpora\u001b[39m.\u001b[39mDictionary(documents)\n",
      "Cell \u001b[0;32mIn[4], line 4\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgensim\u001b[39;00m \u001b[39mimport\u001b[39;00m corpora, models\n\u001b[1;32m      3\u001b[0m \u001b[39m# Create a list of tokenized documents\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m documents \u001b[39m=\u001b[39m [doc[\u001b[39m'\u001b[39;49m\u001b[39mclean text\u001b[39;49m\u001b[39m'\u001b[39;49m] \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m data]\n\u001b[1;32m      6\u001b[0m \u001b[39m# Create a dictionary of all the unique words in the documents\u001b[39;00m\n\u001b[1;32m      7\u001b[0m dictionary \u001b[39m=\u001b[39m corpora\u001b[39m.\u001b[39mDictionary(documents)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'clean text'"
     ]
    }
   ],
   "source": [
    "from gensim import corpora, models\n",
    "\n",
    "# Create a list of tokenized documents\n",
    "documents = [doc['clean text'] for doc in data]\n",
    "\n",
    "# Create a dictionary of all the unique words in the documents\n",
    "dictionary = corpora.Dictionary(documents)\n",
    "\n",
    "# Convert the documents into a document-term matrix\n",
    "corpus = [dictionary.doc2bow(doc) for doc in documents]\n",
    "\n",
    "# Define the number of topics\n",
    "num_topics = 5\n",
    "\n",
    "# Train the LDA model\n",
    "lda_model = models.LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=10)\n",
    "\n",
    "# Get the top five words for each topic\n",
    "top_words_per_topic = []\n",
    "for topic_id in range(num_topics):\n",
    "    top_words = lda_model.show_topic(topic_id, topn=5)\n",
    "    top_words_per_topic.append([word for word, _ in top_words])\n",
    "\n",
    "# Print the top five words for each topic\n",
    "for i, top_words in enumerate(top_words_per_topic):\n",
    "    print(f\"Topic {i + 1}: {' '.join(top_words)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2a3896",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1573fe65",
   "metadata": {},
   "source": [
    "### Extract themes\n",
    "Write a theme for each topic (atleast a sentence each)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9737d8df",
   "metadata": {},
   "source": [
    "[Your Answer]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "21cf24c6",
   "metadata": {},
   "source": [
    "[Your Answer]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "75e191f4",
   "metadata": {},
   "source": [
    "[Your Answer]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ab7df0",
   "metadata": {},
   "source": [
    "### Add Topid ID to output file\n",
    "Add the top two topics for each smart city to the data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e937841",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models\n",
    "\n",
    "# Create a list of tokenized documents\n",
    "documents = [doc['clean text'] for doc in data]\n",
    "\n",
    "# Create a dictionary of all the unique words in the documents\n",
    "dictionary = corpora.Dictionary(documents)\n",
    "\n",
    "# Convert the documents into a document-term matrix\n",
    "corpus = [dictionary.doc2bow(doc) for doc in documents]\n",
    "\n",
    "# Define the number of topics\n",
    "num_topics = <number_of_topics>\n",
    "\n",
    "# Train the LDA model\n",
    "lda_model = models.LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=10)\n",
    "\n",
    "# Get the top two topics for each smart city\n",
    "for doc in data:\n",
    "    # Get the document's bag-of-words representation\n",
    "    doc_bow = dictionary.doc2bow(doc['clean text'])\n",
    "    \n",
    "    # Get the document's topic distribution\n",
    "    doc_topics = lda_model[doc_bow]\n",
    "    \n",
    "    # Sort the topics by their probability in descending order\n",
    "    doc_topics = sorted(doc_topics, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Get the top two topics for the document\n",
    "    top_topics = [topic_id for topic_id, _ in doc_topics[:2]]\n",
    "    \n",
    "    # Add the top topics to the data structure\n",
    "    doc['top_topics'] = top_topics\n",
    "\n",
    "# Print the updated data structure\n",
    "for doc in data:\n",
    "    print(f\"City: {doc['city']}, Top Topics: {doc['top_topics']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e568652",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "39f9c240",
   "metadata": {},
   "source": [
    "## Gathering Applicant Summaries and Keywords\n",
    "\n",
    "For each smart city applicant, gather a summary and keywords that are important to that document. You can use gensim to do this. Here are examples of functions that you could use.\n",
    "\n",
    "```python\n",
    "\n",
    "from gensim.summarization import summarize\n",
    "\n",
    "def summary(text, ratio=0.2, word_count=250, split=False):\n",
    "    return summarize(text, ratio= ratio, word_count=word_count, split=split)\n",
    "    \n",
    "from gensim.summarization import keywords\n",
    "\n",
    "def keys(text, ratio=0.01):\n",
    "    return keywords(text, ratio=ratio)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a86cbe26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Using cached nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "Collecting joblib\n",
      "  Using cached joblib-1.2.0-py3-none-any.whl (297 kB)\n",
      "Collecting regex>=2021.8.3\n",
      "  Downloading regex-2023.5.5-cp39-cp39-macosx_11_0_arm64.whl (288 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.9/288.9 KB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: click in /Users/sagarsingh/.pyenv/versions/3.9.13/lib/python3.9/site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: tqdm in /Users/sagarsingh/.pyenv/versions/3.9.13/lib/python3.9/site-packages (from nltk) (4.65.0)\n",
      "Installing collected packages: regex, joblib, nltk\n",
      "Successfully installed joblib-1.2.0 nltk-3.8.1 regex-2023.5.5\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 23.1.2 is available.\n",
      "You should consider upgrading via the '/Users/sagarsingh/.pyenv/versions/3.9.13/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PyPDF2 import PdfFileReader\n",
    "import re\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def load_and_clean_pdfs(directory):\n",
    "    data = []\n",
    "    cities_to_remove = []\n",
    "\n",
    "    # Iterate through all PDF files in the directory\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".pdf\"):\n",
    "            filepath = os.path.join(directory, filename)\n",
    "\n",
    "            # Extract city name from file name\n",
    "            city = filename[:-4]\n",
    "\n",
    "            # Extract raw text from PDF file\n",
    "            with open(filepath, \"rb\") as f:\n",
    "                pdf_reader = PdfFileReader(f)\n",
    "                raw_text = \"\"\n",
    "                for i in range(pdf_reader.getNumPages()):\n",
    "                    page = pdf_reader.getPage(i)\n",
    "                    raw_text += page.extractText()\n",
    "\n",
    "            # Clean raw text\n",
    "            clean_text = normalize_text(raw_text)\n",
    "\n",
    "            # Check if text was not processed correctly\n",
    "            if clean_text.strip() == \"\":\n",
    "                cities_to_remove.append(city)\n",
    "                continue\n",
    "\n",
    "            # Remove terms that may affect clustering and topic modeling\n",
    "            clean_text = remove_terms(clean_text)\n",
    "\n",
    "            # Add city name, raw text, and clean text to data list\n",
    "            data.append({\"city\": city, \"raw text\": raw_text, \"clean text\": clean_text})\n",
    "\n",
    "            # Check if more than 15 cities have been removed\n",
    "            if len(cities_to_remove) > 15:\n",
    "                break\n",
    "\n",
    "    # Remove cities that were not processed correctly\n",
    "    data = [d for d in data if d[\"city\"] not in cities_to_remove]\n",
    "\n",
    "    return data\n",
    "\n",
    "def normalize_text(text):\n",
    "    contraction_map = {\n",
    "        # Define your contraction mapping here\n",
    "    }\n",
    "\n",
    "    contraction_pattern = re.compile('({})'.format('|'.join(contraction_map.keys())),\n",
    "                                     flags=re.IGNORECASE | re.DOTALL)\n",
    "\n",
    "    def expand_match(contraction):\n",
    "        match = contraction.group(0)\n",
    "        first_char = match[0]\n",
    "        expanded_contraction = contraction_map.get(match) if contraction_map.get(match) else contraction_map.get(\n",
    "            match.lower())\n",
    "        expanded_contraction = first_char + expanded_contraction[1:]\n",
    "        return expanded_contraction\n",
    "\n",
    "    expanded_text = contraction_pattern.sub(expand_match, text)\n",
    "    expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
    "    return expanded_text\n",
    "\n",
    "def remove_terms(text):\n",
    "    terms_to_remove = ['city', 'state', 'smart', 'page']\n",
    "    for term in terms_to_remove:\n",
    "        text = text.replace(term, \"\")\n",
    "    return text\n",
    "\n",
    "def generate_summary_keywords_spacy(document):\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    doc = nlp(document)\n",
    "    sentences = [sent.text for sent in doc.sents]\n",
    "    summary = \". \".join(sentences[:2])\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    keywords = [token.text.lower() for token in doc if not token.is_stop and not token.is_punct]\n",
    "    return summary, keywords\n",
    "\n",
    "def generate_summary_keywords_nltk(document):\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    sentences = nltk.sent_tokenize(document)\n",
    "    summary = \". \".join(sentences[:2])\n",
    "    keywords = [word.lower() for word in word_tokenize(document) if word.lower() not in stop_words and word.isalnum()]\n",
    "    return summary, keywords\n",
    "\n",
    "# Specify the directory containing the PDF files\n",
    "directory = \"smartcity\"\n",
    "\n",
    "# Load and clean the PDFs\n",
    "data = load_and_clean_pdfs(directory)\n",
    "\n",
    "# Generate summaries and keywords for each document\n",
    "for doc in data:\n",
    "    document = doc['clean text']\n",
    "    summary, keywords = generate_summary_keywords_spacy(document) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b545ac4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2d27ce37",
   "metadata": {},
   "source": [
    "### Add Summaries and Keywords\n",
    "Add summary and keywords to output file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09357ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the directory containing the PDF files\n",
    "directory = \"smartcity\"\n",
    "\n",
    "# Load and clean the PDFs\n",
    "data = load_and_clean_pdfs(directory)\n",
    "\n",
    "# Generate summaries and keywords for each document\n",
    "for doc in data:\n",
    "    document = doc['clean text']\n",
    "    summary, keywords = generate_summary_keywords_spacy(document)\n",
    "    \n",
    "    # Add summary and keywords to the data structure\n",
    "    doc['summary'] = summary\n",
    "    doc['keywords'] = keywords\n",
    "    \n",
    "    # Append summary and keywords to the output file\n",
    "    with open(\"output.txt\", \"a\") as f:\n",
    "        f.write(f\"City: {doc['city']}\\n\")\n",
    "        f.write(f\"Summary: {summary}\\n\")\n",
    "        f.write(f\"Keywords: {', '.join(keywords)}\\n\\n\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4f9cb2c4",
   "metadata": {},
   "source": [
    "## Write output data\n",
    "\n",
    "The output data should be written as a TSV file.\n",
    "You can use `to_csv` method from Pandas for this if you are using a DataFrame.\n",
    "\n",
    "`Syntax: df.to_csv('file.tsv', sep = '')` \\\n",
    "`df.to_csv('smartcity_eda.tsv', sep='\\t')`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58827464",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert the data to a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Specify the output file path\n",
    "output_file = 'smartcity_eda.tsv'\n",
    "\n",
    "# Write the DataFrame to a TSV file\n",
    "df.to_csv(output_file, sep='\\t', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a18b8ff",
   "metadata": {},
   "source": [
    "# Moving Forward\n",
    "Now that you have explored the dataset, take the important features and functions to create your `project3.py`.\n",
    "Please refer to the project spec for more guidance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6675ba",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (main, Feb 25 2023, 13:51:03) \n[Clang 14.0.0 (clang-1400.0.29.202)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "8402b68c9fe91dfd2fb5b88e506761fd689e16a64c0b85dee91d955d7dd98c8a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
